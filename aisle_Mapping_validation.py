"""
Python code extracted from aisle_Mapping_validation.ipynb
Generated by Project Publisher

Original notebook: 22 cells
Code cells: 22
"""

# Imports
from google.colab import drive
import json
import openai
import os
import pandas as pd

# Main code
drive.mount('/content/drive')
!pip install -r "/content/drive/MyDrive/LLMProjects/requirements.txt"
# Read and set the environment variable from the .bashrc file
with open('/content/drive/MyDrive/LLMProjects/.bashrc') as file:
    for line in file:
        if line.startswith('export '):
            var, value = line[len('export '):].strip().split('=')
            os.environ[var] = value
# Verify that the environment variable is set
#!echo $OPENAI_API_KEY
#!echo $GROQ_API_KEY
client = openai.OpenAI()
def get_completion(prompt, model="gpt-3.5-turbo"):
    messages = [{"role": "user", "content": prompt}]
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0
    )
    return response.choices[0].message.content
df = pd.read_excel('/content/drive/MyDrive/Aisle-Mapping.xlsx')
groceries = df['Grocery ITEM'].dropna().tolist()
aisles = df['Aisle Category'].dropna().tolist()
batch_size = 50  # Adjust the batch size as needed
separator=","
# Function to get the best matches for a batch of keywords using chat completion
def get_best_matches_batch(grocery_batch, aisle_list):
    prompt = "Match each grocery item in the  grocery list with the most appropriate aisle category from the provided list of aisle categories.\n\n"
    prompt += "The grocery list items are separated by commas. The list of aisles are also separated by commas. \n\n"
    prompt += "List of grocery items to match:\n" + "\n"+ "\n"+separator.join(grocery_batch)+  "\n\n"
    prompt += "List of provided aisle categories:\n" + "\n"+ "\n"+separator.join(aisle_list)+ "\n\n"
    prompt += "If an appropriate aisle category is not to be found in the list, use Other \n\n"
    prompt += "Return the matches in the format 'grocery item -> aisle category \n\n"
    prompt+="You must absolutely make sure that each grocery item is mapped to an aisle category"
    #print(prompt)
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant that matches grocery items to aisle categories based on a typical grocery store or a supermarket in the USA."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=4096,
        temperature=0.0
    )
    matches = response.choices[0].message.content
    return matches
grocery_batch = groceries[0: batch_size]
batch_matches = get_best_matches_batch(grocery_batch, aisles)
print(batch_matches)
# Initialize the clients
client_llama = groq.Client()
client_gpt = openai.OpenAI()
# Function to create JSON output
def generate_aisle_mapping(items, model, client):
    json_output = {}
    for item in items:
        prompt = f"Please categorize the grocery item '{item}' into an aisle."
        aisle_category = get_completion(prompt, model, client)
        json_output[item] = aisle_category
    return json_output
model_llama = "llama3-70b-8192"
model_gpt = "gpt-3.5-turbo"
# Generate JSON outputs
json_llama = generate_aisle_mapping(groceries[:batch_size], model_llama, client_llama) # Pass the first 'batch_size' grocery items from the 'groceries' list
json_gpt = generate_aisle_mapping(groceries[:batch_size], model_gpt, client_gpt)
# Save JSON outputs
with open('json_llama_output.json', 'w') as f:
    json.dump(json_llama, f)
with open('json_gpt_output.json', 'w') as f:
    json.dump(json_gpt, f)
# Load the JSON output files
with open('json_llama_output.json') as f:
    json_llama = json.load(f)
with open('json_gpt_output.json') as f:
    json_gpt = json.load(f)
def validate_aisle_mapping(json_output, model, client):
    validation_results = {}
    for item, aisle in json_output.items():
        prompt = f"Is the item '{item}' correctly categorized in the aisle '{aisle}'? Answer 'yes' or 'no'."
        validation_response = get_completion(prompt, model, client)
        validation_results[item] = validation_response
    return validation_results
# Choose a model for validation
#validation_model =  "llama3-70b-8192"
validation_model =  "gpt-3.5-turbo"
# Validate JSON outputs
validation_results_llama = validate_aisle_mapping(json_llama, validation_model, client_gpt)  # Assuming using GPT for validation
validation_results_gpt = validate_aisle_mapping(json_gpt, validation_model, client_gpt)  # Assuming using GPT for validation
# Display validation results
print("\nValidation Results for Llama JSON Output:")
for item, result in validation_results_llama.items():
   print(f"{item} -> {result}")
print("\nValidation Results for GPT JSON Output:")
for item, result in validation_results_gpt.items():
    print(f"{item} -> {result}")
# Save validation results
with open('validation_results_llama.json', 'w') as f:
    json.dump(validation_results_llama, f)
with open('validation_results_gpt.json', 'w') as f:
    json.dump(validation_results_gpt, f)
# Observations
correct_llama = sum(1 for result in validation_results_llama.values() if 'yes' in result.lower())
incorrect_llama = len(validation_results_llama) - correct_llama
correct_gpt = sum(1 for result in validation_results_gpt.values() if 'yes' in result.lower())
incorrect_gpt = len(validation_results_gpt) - correct_gpt
print(f"Llama model: {correct_llama} correct, {incorrect_llama} incorrect")
print(f"GPT model: {correct_gpt} correct, {incorrect_gpt} incorrect")
prompt = f"""
According to your knowledge, does 'butter' belong in the 'Dairy' aisle? Type 'yes' if it's accurate, 'no' otherwise
"""
response = get_completion(prompt, model_gpt, client)
print(response)
prompt = f"""
Suggest a category for pip fruit and fruit/vegetable juice
"""
response = get_completion(prompt, model_gpt, client)
print(response)

if __name__ == "__main__":
    # Run the main functionality
    print("Notebook code extracted successfully")