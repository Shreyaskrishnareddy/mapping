# ğŸ““ Jupyter Notebook: aisle_Mapping_validation.ipynb

**Notebook Overview:**
- Total Cells: 22
- Code Cells: 22
- Markdown Cells: 0

**Kernel:** Python 3

**Language:** python Unknown

## ğŸ“‹ Notebook Content:

### ğŸ”¹ Code Cell 1 [Execution: 1]

```python
from google.colab import drive
drive.mount('/content/drive')
```

**Output:**
```
Mounted at /content/drive
```

---

### ğŸ”¹ Code Cell 2 [Execution: 2]

```python
!pip install -r "/content/drive/MyDrive/LLMProjects/requirements.txt"
```

**Output:**
```
Collecting openai (from -r /content/drive/MyDrive/LLMProjects/requirements.txt (line 1))
  Downloading openai-1.35.7-py3-none-any.whl (327 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m327.5/327.5 kB[0m [31m2.2 MB/s[0m eta [36m0:00:00[0m
[?25hCollecting langchain (from -r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2))
  Downloading langchain-0.2.6-py3-none-any.whl (975 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m975.5/975.5 kB[0m [31m14.5 MB/s[0m eta [36m0:00:00[0m
[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 1)) (3.7.1)
Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 1)) (1.7.0)
Collecting httpx<1,>=0.23.0 (from openai->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 1))
  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m75.6/75.6 kB[0m [31m8.4 MB/s[0m eta [36m0:00:00[0m
[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 1)) (2.7.4)
Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 1)) (1.3.1)
Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 1)) (4.66.4)
Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 1)) (4.12.2)
Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2)) (6.0.1)
Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2)) (2.0.31)
Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2)) (3.9.5)
Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2)) (4.0.3)
Collecting langchain-core<0.3.0,>=0.2.10 (from langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2))
  Downloading langchain_core-0.2.10-py3-none-any.whl (332 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m332.8/332.8 kB[0m [31m30.4 MB/s[0m eta [36m0:00:00[0m
[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2))
  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)
Collecting langsmith<0.2.0,>=0.1.17 (from langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2))
  Downloading langsmith-0.1.82-py3-none-any.whl (127 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m127.4/127.4 kB[0m [31m5.2 MB/s[0m eta [36m0:00:00[0m
[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2)) (1.25.2)
Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2)) (2.31.0)
Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2)) (8.4.1)
Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2)) (1.3.1)
Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2)) (23.2.0)
Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2)) (1.4.1)
Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2)) (6.0.5)
Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2)) (1.9.4)
Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 1)) (3.7)
Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 1)) (1.2.1)
Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 1)) (2024.6.2)
Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 1))
  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m77.9/77.9 kB[0m [31m8.7 MB/s[0m eta [36m0:00:00[0m
[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 1))
  Downloading h11-0.14.0-py3-none-any.whl (58 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m58.3/58.3 kB[0m [31m6.7 MB/s[0m eta [36m0:00:00[0m
[?25hCollecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.10->langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2))
  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)
Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2)) (24.1)
Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2))
  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m145.0/145.0 kB[0m [31m13.0 MB/s[0m eta [36m0:00:00[0m
[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 1)) (0.7.0)
Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 1)) (2.18.4)
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2)) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2)) (2.0.7)
Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2)) (3.0.3)
Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain->-r /content/drive/MyDrive/LLMProjects/requirements.txt (line 2))
  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)
Installing collected packages: orjson, jsonpointer, h11, jsonpatch, httpcore, langsmith, httpx, openai, langchain-core, langchain-text-splitters, langchain
Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.6 langchain-core-0.2.10 langchain-text-splitters-0.2.2 langsmith-0.1.82 openai-1.35.7 orjson-3.10.5
```

---

### ğŸ”¹ Code Cell 3 [Execution: 4]

```python
import os

# Read and set the environment variable from the .bashrc file
with open('/content/drive/MyDrive/LLMProjects/.bashrc') as file:
    for line in file:
        if line.startswith('export '):
            var, value = line[len('export '):].strip().split('=')
            os.environ[var] = value

# Verify that the environment variable is set
#!echo $OPENAI_API_KEY
#!echo $GROQ_API_KEY
```

---

### ğŸ”¹ Code Cell 4 [Execution: 5]

```python
import openai

client = openai.OpenAI()

def get_completion(prompt, model="gpt-3.5-turbo"):
    messages = [{"role": "user", "content": prompt}]
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0
    )
    return response.choices[0].message.content
```

---

### ğŸ”¹ Code Cell 5 [Execution: 6]

```python
import pandas as pd
```

---

### ğŸ”¹ Code Cell 6

```python
df = pd.read_excel('/content/drive/MyDrive/Aisle-Mapping.xlsx')
```

---

### ğŸ”¹ Code Cell 7

```python
groceries = df['Grocery ITEM'].dropna().tolist()
aisles = df['Aisle Category'].dropna().tolist()
```

---

### ğŸ”¹ Code Cell 8

```python
batch_size = 50  # Adjust the batch size as needed
separator=","

# Function to get the best matches for a batch of keywords using chat completion
def get_best_matches_batch(grocery_batch, aisle_list):
    prompt = "Match each grocery item in the  grocery list with the most appropriate aisle category from the provided list of aisle categories.\n\n"
    prompt += "The grocery list items are separated by commas. The list of aisles are also separated by commas. \n\n"
    prompt += "List of grocery items to match:\n" + "\n"+ "\n"+separator.join(grocery_batch)+  "\n\n"
    prompt += "List of provided aisle categories:\n" + "\n"+ "\n"+separator.join(aisle_list)+ "\n\n"
    prompt += "If an appropriate aisle category is not to be found in the list, use Other \n\n"
    prompt += "Return the matches in the format 'grocery item -> aisle category \n\n"
    prompt+="You must absolutely make sure that each grocery item is mapped to an aisle category"
    #print(prompt)
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant that matches grocery items to aisle categories based on a typical grocery store or a supermarket in the USA."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=4096,
        temperature=0.0
    )
    matches = response.choices[0].message.content
    return matches
```

---

### ğŸ”¹ Code Cell 9

```python
grocery_batch = groceries[0: batch_size]
batch_matches = get_best_matches_batch(grocery_batch, aisles)
print(batch_matches)
```

**Output:**
```
tropical fruit -> Produce
whole milk -> Dairy
pip fruit -> Produce
other vegetables -> Produce
whole milk -> Dairy
rolls/buns -> Bakery
other vegetables -> Produce
pot plants -> Garden
whole milk -> Dairy
tropical fruit -> Produce
citrus fruit -> Produce
beef -> Meat
frankfurter -> Meat
chicken -> Meat
butter -> Dairy
fruit/vegetable juice -> Beverages
packaged fruit/vegetables -> Produce
chocolate -> Candy
specialty bar -> Candy
other vegetables -> Produce
butter milk -> Dairy
whole milk -> Dairy
tropical fruit -> Produce
tropical fruit -> Produce
bottled water -> Beverages
yogurt -> Dairy
sausage -> Meat
other vegetables -> Produce
brown bread -> Bakery
yogurt -> Dairy
hamburger meat -> Meat
root vegetables -> Produce
pork -> Meat
beef -> Meat
pastry -> Bakery
fruit/vegetable juice -> Beverages
canned beer -> Alcohol
root vegetables -> Produce
citrus fruit -> Produce
sausage -> Meat
tropical fruit -> Produce
berries -> Produce
canned beer -> Alcohol
butter milk -> Dairy
coffee -> Beverages
pastry -> Bakery
rolls/buns -> Bakery
misc. beverages -> Beverages
root vegetables -> Produce
sausage -> Meat
```

---

### ğŸ”¹ Code Cell 10

```python
# Initialize the clients
client_llama = groq.Client()
client_gpt = openai.OpenAI()
```

---

### ğŸ”¹ Code Cell 11

```python
# Function to create JSON output
def generate_aisle_mapping(items, model, client):
    json_output = {}
    for item in items:
        prompt = f"Please categorize the grocery item '{item}' into an aisle."
        aisle_category = get_completion(prompt, model, client)
        json_output[item] = aisle_category
    return json_output
```

---

### ğŸ”¹ Code Cell 12

```python
model_llama = "llama3-70b-8192"
model_gpt = "gpt-3.5-turbo"
```

---

### ğŸ”¹ Code Cell 13

```python
# Generate JSON outputs
json_llama = generate_aisle_mapping(groceries[:batch_size], model_llama, client_llama) # Pass the first 'batch_size' grocery items from the 'groceries' list
json_gpt = generate_aisle_mapping(groceries[:batch_size], model_gpt, client_gpt)
```

---

### ğŸ”¹ Code Cell 14

```python
# Save JSON outputs
import json
with open('json_llama_output.json', 'w') as f:
    json.dump(json_llama, f)
with open('json_gpt_output.json', 'w') as f:
    json.dump(json_gpt, f)
```

---

### ğŸ”¹ Code Cell 15

```python
# Load the JSON output files
with open('json_llama_output.json') as f:
    json_llama = json.load(f)
with open('json_gpt_output.json') as f:
    json_gpt = json.load(f)


def validate_aisle_mapping(json_output, model, client):
    validation_results = {}
    for item, aisle in json_output.items():
        prompt = f"Is the item '{item}' correctly categorized in the aisle '{aisle}'? Answer 'yes' or 'no'."
        validation_response = get_completion(prompt, model, client)
        validation_results[item] = validation_response
    return validation_results
```

---

### ğŸ”¹ Code Cell 16

```python
# Choose a model for validation
#validation_model =  "llama3-70b-8192"
validation_model =  "gpt-3.5-turbo"
```

---

### ğŸ”¹ Code Cell 17

```python
# Validate JSON outputs
validation_results_llama = validate_aisle_mapping(json_llama, validation_model, client_gpt)  # Assuming using GPT for validation
validation_results_gpt = validate_aisle_mapping(json_gpt, validation_model, client_gpt)  # Assuming using GPT for validation
```

---

### ğŸ”¹ Code Cell 18

```python
# Display validation results
print("\nValidation Results for Llama JSON Output:")
for item, result in validation_results_llama.items():
   print(f"{item} -> {result}")

print("\nValidation Results for GPT JSON Output:")
for item, result in validation_results_gpt.items():
    print(f"{item} -> {result}")
```

**Output:**
```
Validation Results for Llama JSON Output:
tropical fruit -> Yes
whole milk -> Yes
pip fruit -> No
other vegetables -> Yes
rolls/buns -> Yes
pot plants -> Yes
citrus fruit -> Yes
beef -> Yes
frankfurter -> Yes
chicken -> Yes
butter -> Yes
fruit/vegetable juice -> Yes
packaged fruit/vegetables -> Yes
chocolate -> Yes
specialty bar -> Yes
butter milk -> Yes
bottled water -> Yes
yogurt -> Yes
sausage -> Yes
brown bread -> Yes
hamburger meat -> Yes
root vegetables -> Yes
pork -> Yes
pastry -> Yes
canned beer -> No
berries -> Yes
coffee -> Yes
misc. beverages -> Yes

Validation Results for GPT JSON Output:
tropical fruit -> Yes
whole milk -> Yes
pip fruit -> Yes
other vegetables -> Yes
rolls/buns -> Yes
pot plants -> Yes
citrus fruit -> Yes
beef -> Yes
frankfurter -> Yes
chicken -> Yes
butter -> Yes
fruit/vegetable juice -> No
packaged fruit/vegetables -> Yes
chocolate -> Yes
specialty bar -> Yes
butter milk -> Yes
bottled water -> Yes
yogurt -> Yes
sausage -> Yes
brown bread -> Yes
hamburger meat -> Yes
root vegetables -> Yes
pork -> Yes
pastry -> Yes
canned beer -> Yes
berries -> Yes
coffee -> Yes
misc. beverages -> Yes
```

---

### ğŸ”¹ Code Cell 19

```python
# Save validation results
with open('validation_results_llama.json', 'w') as f:
    json.dump(validation_results_llama, f)
with open('validation_results_gpt.json', 'w') as f:
    json.dump(validation_results_gpt, f)
```

---

### ğŸ”¹ Code Cell 20

```python
# Observations
correct_llama = sum(1 for result in validation_results_llama.values() if 'yes' in result.lower())
incorrect_llama = len(validation_results_llama) - correct_llama

correct_gpt = sum(1 for result in validation_results_gpt.values() if 'yes' in result.lower())
incorrect_gpt = len(validation_results_gpt) - correct_gpt

print(f"Llama model: {correct_llama} correct, {incorrect_llama} incorrect")
print(f"GPT model: {correct_gpt} correct, {incorrect_gpt} incorrect")
```

**Output:**
```
Llama model: 26 correct, 2 incorrect
GPT model: 27 correct, 1 incorrect
```

---

### ğŸ”¹ Code Cell 21

```python
prompt = f"""
According to your knowledge, does 'butter' belong in the 'Dairy' aisle? Type 'yes' if it's accurate, 'no' otherwise
"""

response = get_completion(prompt, model_gpt, client)
print(response)
```

**Output:**
```
Yes
```

---

### ğŸ”¹ Code Cell 22

```python
prompt = f"""
Suggest a category for pip fruit and fruit/vegetable juice
"""

response = get_completion(prompt, model_gpt, client)
print(response)
```

**Output:**
```
Produce or Fresh Produce
```

---
